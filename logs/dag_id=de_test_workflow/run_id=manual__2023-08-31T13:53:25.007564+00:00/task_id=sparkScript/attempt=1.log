[2023-08-31T15:53:27.138+0200] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: de_test_workflow.sparkScript manual__2023-08-31T13:53:25.007564+00:00 [queued]>
[2023-08-31T15:53:27.143+0200] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: de_test_workflow.sparkScript manual__2023-08-31T13:53:25.007564+00:00 [queued]>
[2023-08-31T15:53:27.143+0200] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2023-08-31T15:53:27.150+0200] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): sparkScript> on 2023-08-31 13:53:25.007564+00:00
[2023-08-31T15:53:27.152+0200] {standard_task_runner.py:57} INFO - Started process 3402 to run task
[2023-08-31T15:53:27.154+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'de_test_workflow', 'sparkScript', 'manual__2023-08-31T13:53:25.007564+00:00', '--job-id', '5', '--raw', '--subdir', '/Users/france.cama/code/glovo_de_test/dags/workflows.py', '--cfg-path', '/var/folders/bq/3fzm19j96pq0b42q_ghqb6j40000gn/T/tmpnc_2d2uw']
[2023-08-31T15:53:27.155+0200] {standard_task_runner.py:85} INFO - Job 5: Subtask sparkScript
[2023-08-31T15:53:27.195+0200] {task_command.py:415} INFO - Running <TaskInstance: de_test_workflow.sparkScript manual__2023-08-31T13:53:25.007564+00:00 [running]> on host mba-francesco.local
[2023-08-31T15:53:27.246+0200] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='francesco' AIRFLOW_CTX_DAG_ID='de_test_workflow' AIRFLOW_CTX_TASK_ID='sparkScript' AIRFLOW_CTX_EXECUTION_DATE='2023-08-31T13:53:25.007564+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-08-31T13:53:25.007564+00:00'
[2023-08-31T15:53:27.256+0200] {base.py:73} INFO - Using connection ID 'spark_standalone_conn' for task execution.
[2023-08-31T15:53:27.257+0200] {spark_submit.py:340} INFO - Spark-Submit cmd: spark-submit --master local --executor-memory 3g --name arrow-spark ./app/main.py
[2023-08-31T15:53:28.209+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 WARN Utils: Your hostname, MBA-Francesco.local resolves to a loopback address: 127.0.0.1; using 192.168.1.62 instead (on interface en0)
[2023-08-31T15:53:28.210+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-08-31T15:53:28.764+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO SparkContext: Running Spark version 3.4.1
[2023-08-31T15:53:28.797+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-08-31T15:53:28.859+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO ResourceUtils: ==============================================================
[2023-08-31T15:53:28.860+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-08-31T15:53:28.860+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO ResourceUtils: ==============================================================
[2023-08-31T15:53:28.860+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO SparkContext: Submitted application: arrow-spark
[2023-08-31T15:53:28.867+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 2048, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-08-31T15:53:28.871+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO ResourceProfile: Limiting resource is cpu
[2023-08-31T15:53:28.871+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-08-31T15:53:28.894+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO SecurityManager: Changing view acls to: france.cama
[2023-08-31T15:53:28.895+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO SecurityManager: Changing modify acls to: france.cama
[2023-08-31T15:53:28.895+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO SecurityManager: Changing view acls groups to:
[2023-08-31T15:53:28.895+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO SecurityManager: Changing modify acls groups to:
[2023-08-31T15:53:28.895+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: france.cama; groups with view permissions: EMPTY; users with modify permissions: france.cama; groups with modify permissions: EMPTY
[2023-08-31T15:53:28.982+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO Utils: Successfully started service 'sparkDriver' on port 54815.
[2023-08-31T15:53:28.995+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:28 INFO SparkEnv: Registering MapOutputTracker
[2023-08-31T15:53:29.009+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO SparkEnv: Registering BlockManagerMaster
[2023-08-31T15:53:29.016+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-08-31T15:53:29.016+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-08-31T15:53:29.017+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-08-31T15:53:29.029+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO DiskBlockManager: Created local directory at /private/var/folders/bq/3fzm19j96pq0b42q_ghqb6j40000gn/T/blockmgr-a944dc52-bdff-4709-a34b-3358e368cca8
[2023-08-31T15:53:29.040+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO MemoryStore: MemoryStore started with capacity 2.4 GiB
[2023-08-31T15:53:29.046+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-08-31T15:53:29.100+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2023-08-31T15:53:29.121+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-08-31T15:53:29.164+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO Executor: Starting executor ID driver on host 192.168.1.62
[2023-08-31T15:53:29.166+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-08-31T15:53:29.172+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54816.
[2023-08-31T15:53:29.173+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO NettyBlockTransferService: Server created on 192.168.1.62:54816
[2023-08-31T15:53:29.173+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-08-31T15:53:29.175+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.62, 54816, None)
[2023-08-31T15:53:29.177+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.62:54816 with 2.4 GiB RAM, BlockManagerId(driver, 192.168.1.62, 54816, None)
[2023-08-31T15:53:29.177+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.62, 54816, None)
[2023-08-31T15:53:29.178+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.62, 54816, None)
[2023-08-31T15:53:29.356+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-08-31T15:53:29.359+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO SharedState: Warehouse path is 'file:/Users/france.cama/code/glovo_de_test/spark-warehouse'.
[2023-08-31T15:53:29.723+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.
[2023-08-31T15:53:29.749+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:29 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2023-08-31T15:53:30.513+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:30 INFO FileSourceStrategy: Pushed Filters:
[2023-08-31T15:53:30.513+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:30 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2023-08-31T15:53:30.704+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:30 INFO CodeGenerator: Code generated in 78.923708 ms
[2023-08-31T15:53:30.721+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.188+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.189+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.62:54816 (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.191+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:31.194+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4362002 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:31.239+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:31.247+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:31.248+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:31.248+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:31.248+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:31.249+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:31.272+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.272+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.273+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.62:54816 (size: 6.0 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.273+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:31.278+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:31.278+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-08-31T15:53:31.293+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7948 bytes)
[2023-08-31T15:53:31.298+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-08-31T15:53:31.329+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/orders.csv, range: 0-167698, partition values: [empty row]
[2023-08-31T15:53:31.336+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO CodeGenerator: Code generated in 4.856 ms
[2023-08-31T15:53:31.355+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1649 bytes result sent to driver
[2023-08-31T15:53:31.358+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 68 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:31.359+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-08-31T15:53:31.361+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0,108 s
[2023-08-31T15:53:31.362+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:31.362+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-08-31T15:53:31.362+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0,123354 s
[2023-08-31T15:53:31.370+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO CodeGenerator: Code generated in 4.139375 ms
[2023-08-31T15:53:31.386+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceStrategy: Pushed Filters:
[2023-08-31T15:53:31.386+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-08-31T15:53:31.390+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.399+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.400+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.62:54816 (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.400+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:31.400+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4362002 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:31.402+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.62:54816 in memory (size: 6.0 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.430+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:31.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:31.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:31.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:31.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:31.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:31.445+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.446+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.446+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.62:54816 (size: 12.2 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.446+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:31.447+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:31.447+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-08-31T15:53:31.447+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7948 bytes)
[2023-08-31T15:53:31.448+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-08-31T15:53:31.458+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/orders.csv, range: 0-167698, partition values: [empty row]
[2023-08-31T15:53:31.572+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1631 bytes result sent to driver
[2023-08-31T15:53:31.573+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 126 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:31.573+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-08-31T15:53:31.573+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0,142 s
[2023-08-31T15:53:31.573+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:31.574+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-08-31T15:53:31.574+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0,143535 s
[2023-08-31T15:53:31.590+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2023-08-31T15:53:31.593+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2023-08-31T15:53:31.614+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceStrategy: Pushed Filters:
[2023-08-31T15:53:31.614+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#35, None)) > 0)
[2023-08-31T15:53:31.619+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.641+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.642+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.62:54816 (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.642+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:31.642+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 29679345 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:31.648+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:31.648+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:31.648+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:31.649+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:31.649+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:31.649+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:31.650+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.1 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.651+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.651+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.62:54816 (size: 6.0 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.651+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:31.651+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:31.651+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-08-31T15:53:31.652+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7949 bytes)
[2023-08-31T15:53:31.652+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-08-31T15:53:31.658+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/polling.csv, range: 0-25485041, partition values: [empty row]
[2023-08-31T15:53:31.661+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1617 bytes result sent to driver
[2023-08-31T15:53:31.662+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 10 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:31.662+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-08-31T15:53:31.662+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0,013 s
[2023-08-31T15:53:31.663+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:31.663+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-08-31T15:53:31.663+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0,014622 s
[2023-08-31T15:53:31.668+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceStrategy: Pushed Filters:
[2023-08-31T15:53:31.668+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-08-31T15:53:31.675+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.62:54816 in memory (size: 12.2 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.676+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.678+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.62:54816 in memory (size: 6.0 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.686+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.62:54816 in memory (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.691+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.692+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.62:54816 (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.694+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Created broadcast 6 from csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:31.694+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 29679345 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:31.713+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:31.713+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:31.713+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:31.714+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:31.714+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:31.714+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:31.716+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 26.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.716+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 2.4 GiB)
[2023-08-31T15:53:31.718+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.62:54816 (size: 12.3 KiB, free: 2.4 GiB)
[2023-08-31T15:53:31.718+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:31.718+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:31.718+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-08-31T15:53:31.718+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7949 bytes)
[2023-08-31T15:53:31.719+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-08-31T15:53:31.722+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:31 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/polling.csv, range: 0-25485041, partition values: [empty row]
[2023-08-31T15:53:32.053+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.62:54816 in memory (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.218+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1679 bytes result sent to driver
[2023-08-31T15:53:32.219+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 501 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:32.219+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-08-31T15:53:32.219+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0,505 s
[2023-08-31T15:53:32.219+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:32.220+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-08-31T15:53:32.220+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0,506915 s
[2023-08-31T15:53:32.231+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2023-08-31T15:53:32.233+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2023-08-31T15:53:32.251+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters:
[2023-08-31T15:53:32.251+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#75, None)) > 0)
[2023-08-31T15:53:32.256+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.264+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.265+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.62:54816 (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.265+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO SparkContext: Created broadcast 8 from csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:32.265+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6832234 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:32.269+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:32.269+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Got job 4 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:32.269+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:32.269+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:32.270+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:32.270+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:32.270+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.1 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.271+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.271+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.62:54816 (size: 6.0 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.271+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:32.271+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:32.271+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-08-31T15:53:32.272+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7961 bytes)
[2023-08-31T15:53:32.272+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-08-31T15:53:32.273+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/connectivity_status.csv, range: 0-2637930, partition values: [empty row]
[2023-08-31T15:53:32.276+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1615 bytes result sent to driver
[2023-08-31T15:53:32.277+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 5 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:32.277+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-08-31T15:53:32.277+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 0,007 s
[2023-08-31T15:53:32.278+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:32.278+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-08-31T15:53:32.278+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Job 4 finished: csv at NativeMethodAccessorImpl.java:0, took 0,008729 s
[2023-08-31T15:53:32.282+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters:
[2023-08-31T15:53:32.283+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters:
[2023-08-31T15:53:32.285+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 200.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.293+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.294+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.1.62:54816 (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.294+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO SparkContext: Created broadcast 10 from csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:32.294+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6832234 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:32.300+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:32.300+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:32.300+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Final stage: ResultStage 5 (csv at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:32.300+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:32.300+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:32.301+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:32.301+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 26.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.302+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.302+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.1.62:54816 (size: 12.3 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.302+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:32.303+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:32.303+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-08-31T15:53:32.303+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7961 bytes)
[2023-08-31T15:53:32.303+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-08-31T15:53:32.306+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/connectivity_status.csv, range: 0-2637930, partition values: [empty row]
[2023-08-31T15:53:32.326+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.1.62:54816 in memory (size: 6.0 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.327+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.62:54816 in memory (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.328+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.1.62:54816 in memory (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.329+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.1.62:54816 in memory (size: 12.3 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.342+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1717 bytes result sent to driver
[2023-08-31T15:53:32.342+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 39 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:32.343+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-08-31T15:53:32.344+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: ResultStage 5 (csv at NativeMethodAccessorImpl.java:0) finished in 0,042 s
[2023-08-31T15:53:32.344+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:32.345+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-08-31T15:53:32.345+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0,044353 s
[2023-08-31T15:53:32.444+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:32.445+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, device_id#20),isnotnull(device_id#20)
[2023-08-31T15:53:32.445+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:32.445+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#53),atleastnnonnulls(1, device_id#54),isnotnull(device_id#54)
[2023-08-31T15:53:32.446+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:32.446+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, device_id#95),isnotnull(device_id#95)
[2023-08-31T15:53:32.741+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.1.62:54816 in memory (size: 12.3 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.742+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.1.62:54816 in memory (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:32.901+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id),IsNotNull(order_id)
[2023-08-31T15:53:32.902+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, device_id#20),isnotnull(device_id#20),isnotnull(order_id#19)
[2023-08-31T15:53:32.902+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:32.902+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#53),atleastnnonnulls(1, device_id#54),isnotnull(device_id#54)
[2023-08-31T15:53:32.902+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:32.902+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, device_id#95),isnotnull(device_id#95)
[2023-08-31T15:53:32.904+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id),IsNotNull(order_id)
[2023-08-31T15:53:32.905+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, device_id#720),isnotnull(device_id#720),isnotnull(order_id#719)
[2023-08-31T15:53:32.905+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:32.905+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#722),atleastnnonnulls(1, device_id#723),isnotnull(device_id#723)
[2023-08-31T15:53:32.905+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:32.905+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, device_id#729),isnotnull(device_id#729)
[2023-08-31T15:53:32.906+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id),IsNotNull(order_id)
[2023-08-31T15:53:32.906+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, device_id#789),isnotnull(device_id#789),isnotnull(order_id#788)
[2023-08-31T15:53:32.906+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:32.907+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#791),atleastnnonnulls(1, device_id#792),isnotnull(device_id#792)
[2023-08-31T15:53:32.907+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:32.907+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, device_id#798),isnotnull(device_id#798)
[2023-08-31T15:53:32.954+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-08-31T15:53:32.991+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO CodeGenerator: Code generated in 5.031333 ms
[2023-08-31T15:53:32.993+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO CodeGenerator: Code generated in 4.320417 ms
[2023-08-31T15:53:32.994+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.1 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.994+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 200.1 KiB, free 2.4 GiB)
[2023-08-31T15:53:32.994+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO CodeGenerator: Code generated in 5.586667 ms
[2023-08-31T15:53:32.995+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:32 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.1 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.003+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.003+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.1.62:54816 (size: 34.4 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.004+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:33.005+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.006+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.1.62:54816 (size: 34.4 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.006+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:33.007+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4362002 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:33.008+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6832234 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:33.015+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.015+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.1.62:54816 (size: 34.4 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.018+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:33.019+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6832234 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:33.020+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:33.020+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:33.021+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.62:54816 in memory (size: 34.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.021+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) with 1 output partitions
[2023-08-31T15:53:33.022+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317)
[2023-08-31T15:53:33.022+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:33.022+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:33.023+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317), which has no missing parents
[2023-08-31T15:53:33.024+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:33.025+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 14.8 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.025+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.025+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.1.62:54816 (size: 7.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.025+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:33.026+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:33.026+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-08-31T15:53:33.026+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) with 1 output partitions
[2023-08-31T15:53:33.026+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317)
[2023-08-31T15:53:33.026+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:33.026+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:33.026+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317), which has no missing parents
[2023-08-31T15:53:33.027+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7948 bytes)
[2023-08-31T15:53:33.027+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-08-31T15:53:33.027+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 14.2 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.027+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.027+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.1.62:54816 (size: 7.3 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.027+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:33.028+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:33.028+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-08-31T15:53:33.028+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) with 1 output partitions
[2023-08-31T15:53:33.028+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317)
[2023-08-31T15:53:33.028+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:33.028+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:33.028+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[41] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317), which has no missing parents
[2023-08-31T15:53:33.029+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/orders.csv, range: 0-167698, partition values: [empty row]
[2023-08-31T15:53:33.029+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 15.0 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.029+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.030+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.1.62:54816 (size: 7.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.030+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:33.030+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[41] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:33.031+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-08-31T15:53:33.034+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 4.116167 ms
[2023-08-31T15:53:33.049+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 2.786709 ms
[2023-08-31T15:53:33.051+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 1.548959 ms
[2023-08-31T15:53:33.072+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 60446 bytes result sent to driver
[2023-08-31T15:53:33.072+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7961 bytes)
[2023-08-31T15:53:33.073+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-08-31T15:53:33.073+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 48 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:33.073+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-08-31T15:53:33.073+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) finished in 0,052 s
[2023-08-31T15:53:33.074+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:33.074+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-08-31T15:53:33.074+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317, took 0,054053 s
[2023-08-31T15:53:33.074+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/connectivity_status.csv, range: 0-2637930, partition values: [empty row]
[2023-08-31T15:53:33.077+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 1.894083 ms
[2023-08-31T15:53:33.081+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 2.48375 ms
[2023-08-31T15:53:33.106+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 64.1 MiB, free 2.4 GiB)
[2023-08-31T15:53:33.109+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 84.8 KiB, free 2.4 GiB)
[2023-08-31T15:53:33.110+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.1.62:54816 (size: 84.8 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.111+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:33.118+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.1.62:54816 in memory (size: 7.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.125+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.125+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#53),atleastnnonnulls(1, device_id#54),isnotnull(device_id#54)
[2023-08-31T15:53:33.126+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.127+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#722),atleastnnonnulls(1, device_id#723),isnotnull(device_id#723)
[2023-08-31T15:53:33.127+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.127+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#791),atleastnnonnulls(1, device_id#792),isnotnull(device_id#792)
[2023-08-31T15:53:33.145+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 124578 bytes result sent to driver
[2023-08-31T15:53:33.145+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7961 bytes)
[2023-08-31T15:53:33.145+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 73 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:33.146+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-08-31T15:53:33.146+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-08-31T15:53:33.147+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) finished in 0,121 s
[2023-08-31T15:53:33.147+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:33.147+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-08-31T15:53:33.147+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Job 7 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317, took 0,126406 s
[2023-08-31T15:53:33.147+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/connectivity_status.csv, range: 0-2637930, partition values: [empty row]
[2023-08-31T15:53:33.154+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.154+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#53),atleastnnonnulls(1, device_id#54),isnotnull(device_id#54)
[2023-08-31T15:53:33.155+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.155+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#722),atleastnnonnulls(1, device_id#723),isnotnull(device_id#723)
[2023-08-31T15:53:33.158+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 6.395042 ms
[2023-08-31T15:53:33.159+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.160+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#791),atleastnnonnulls(1, device_id#792),isnotnull(device_id#792)
[2023-08-31T15:53:33.181+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 65.0 MiB, free 2.3 GiB)
[2023-08-31T15:53:33.192+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.1.62:54816 in memory (size: 7.3 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.195+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 404.2 KiB, free 2.3 GiB)
[2023-08-31T15:53:33.196+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.1.62:54816 (size: 404.2 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.196+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:33.204+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.205+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#53),atleastnnonnulls(1, device_id#54),isnotnull(device_id#54)
[2023-08-31T15:53:33.205+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.206+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#722),atleastnnonnulls(1, device_id#723),isnotnull(device_id#723)
[2023-08-31T15:53:33.206+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.207+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#791),atleastnnonnulls(1, device_id#792),isnotnull(device_id#792)
[2023-08-31T15:53:33.221+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 469436 bytes result sent to driver
[2023-08-31T15:53:33.222+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 77 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:33.223+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-08-31T15:53:33.224+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) finished in 0,195 s
[2023-08-31T15:53:33.224+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:33.224+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-08-31T15:53:33.224+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317, took 0,200425 s
[2023-08-31T15:53:33.230+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 3.429708 ms
[2023-08-31T15:53:33.259+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 65.0 MiB, free 2.2 GiB)
[2023-08-31T15:53:33.269+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 560.9 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.269+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.1.62:54816 (size: 560.9 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.270+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:33.294+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 26.373625 ms
[2023-08-31T15:53:33.295+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.1 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.299+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.300+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.1.62:54816 (size: 34.4 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.300+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 21 from showString at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:33.300+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 29679345 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:33.323+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Registering RDD 45 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2023-08-31T15:53:33.324+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Got map stage job 9 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:33.324+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (showString at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:33.325+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:33.325+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:33.325+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:33.330+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 64.3 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.330+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.330+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.1.62:54816 (size: 27.4 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.331+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:33.331+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.331+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#53),atleastnnonnulls(1, device_id#54),isnotnull(device_id#54)
[2023-08-31T15:53:33.331+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:33.331+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2023-08-31T15:53:33.332+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(device_id)
[2023-08-31T15:53:33.332+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(1, creation_time#791),atleastnnonnulls(1, device_id#792),isnotnull(device_id#792)
[2023-08-31T15:53:33.332+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7938 bytes)
[2023-08-31T15:53:33.332+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2023-08-31T15:53:33.347+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 4.402709 ms
[2023-08-31T15:53:33.359+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 2.949958 ms
[2023-08-31T15:53:33.364+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 2.6705 ms
[2023-08-31T15:53:33.370+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 4.489208 ms
[2023-08-31T15:53:33.371+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.1.62:54816 in memory (size: 7.5 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.373+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/polling.csv, range: 0-25485041, partition values: [empty row]
[2023-08-31T15:53:33.376+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 2.062292 ms
[2023-08-31T15:53:33.377+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 7.213125 ms
[2023-08-31T15:53:33.377+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 0.984833 ms
[2023-08-31T15:53:33.378+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 200.1 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.392+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.393+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.1.62:54816 (size: 34.4 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.393+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 23 from showString at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:33.393+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 29679345 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:33.395+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Registering RDD 49 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2023-08-31T15:53:33.396+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Got map stage job 10 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:33.396+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (showString at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:33.396+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:33.397+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:33.397+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[49] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:33.397+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 25.6 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.398+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.399+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 192.168.1.62:54816 (size: 11.1 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.399+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:33.399+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[49] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:33.399+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2023-08-31T15:53:33.408+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO CodeGenerator: Code generated in 8.87175 ms
[2023-08-31T15:53:33.409+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 200.1 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.423+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.423+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 192.168.1.62:54816 (size: 34.4 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.424+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 25 from showString at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:33.424+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 29679345 bytes, open cost is considered as scanning 4194304 bytes.
[2023-08-31T15:53:33.428+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Registering RDD 54 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2
[2023-08-31T15:53:33.428+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Got map stage job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:33.428+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (showString at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:33.428+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Parents of final stage: List()
[2023-08-31T15:53:33.428+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:33.429+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[54] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:33.430+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 52.8 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.430+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 2.2 GiB)
[2023-08-31T15:53:33.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 192.168.1.62:54816 (size: 23.6 KiB, free: 2.4 GiB)
[2023-08-31T15:53:33.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:33.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[54] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:33.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:33 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2023-08-31T15:53:34.436+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4265 bytes result sent to driver
[2023-08-31T15:53:34.437+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7938 bytes)
[2023-08-31T15:53:34.440+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2023-08-31T15:53:34.441+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 1108 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:34.441+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2023-08-31T15:53:34.446+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: ShuffleMapStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 1,118 s
[2023-08-31T15:53:34.447+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: looking for newly runnable stages
[2023-08-31T15:53:34.448+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: running: Set(ShuffleMapStage 10, ShuffleMapStage 11)
[2023-08-31T15:53:34.449+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: waiting: Set()
[2023-08-31T15:53:34.449+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: failed: Set()
[2023-08-31T15:53:34.463+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO CodeGenerator: Code generated in 5.490375 ms
[2023-08-31T15:53:34.463+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/polling.csv, range: 0-25485041, partition values: [empty row]
[2023-08-31T15:53:34.469+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO CodeGenerator: Code generated in 6.37 ms
[2023-08-31T15:53:34.493+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-08-31T15:53:34.514+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2023-08-31T15:53:34.532+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO CodeGenerator: Code generated in 11.959916 ms
[2023-08-31T15:53:34.553+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:34.554+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: Got job 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) with 1 output partitions
[2023-08-31T15:53:34.554+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317)
[2023-08-31T15:53:34.555+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
[2023-08-31T15:53:34.556+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:34.556+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[57] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317), which has no missing parents
[2023-08-31T15:53:34.567+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 69.3 KiB, free 2.2 GiB)
[2023-08-31T15:53:34.568+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 2.2 GiB)
[2023-08-31T15:53:34.569+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 192.168.1.62:54816 (size: 30.0 KiB, free: 2.4 GiB)
[2023-08-31T15:53:34.570+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:34.570+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[57] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:34.570+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2023-08-31T15:53:34.692+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:34 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 192.168.1.62:54816 in memory (size: 27.4 KiB, free: 2.4 GiB)
[2023-08-31T15:53:37.471+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2286 bytes result sent to driver
[2023-08-31T15:53:37.473+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (192.168.1.62, executor driver, partition 0, PROCESS_LOCAL, 7938 bytes)
[2023-08-31T15:53:37.473+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 3036 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:37.474+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2023-08-31T15:53:37.474+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2023-08-31T15:53:37.474+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO DAGScheduler: ShuffleMapStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 4,078 s
[2023-08-31T15:53:37.474+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO DAGScheduler: looking for newly runnable stages
[2023-08-31T15:53:37.474+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO DAGScheduler: running: Set(ResultStage 13, ShuffleMapStage 11)
[2023-08-31T15:53:37.474+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO DAGScheduler: waiting: Set()
[2023-08-31T15:53:37.475+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO DAGScheduler: failed: Set()
[2023-08-31T15:53:37.496+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO CodeGenerator: Code generated in 11.186 ms
[2023-08-31T15:53:37.502+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO CodeGenerator: Code generated in 2.284 ms
[2023-08-31T15:53:37.506+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:37 INFO FileScanRDD: Reading File path: file:///Users/france.cama/code/glovo_de_test/test_dataset/polling.csv, range: 0-25485041, partition values: [empty row]
[2023-08-31T15:53:38.191+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:38 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 192.168.1.62:54816 in memory (size: 11.1 KiB, free: 2.4 GiB)
[2023-08-31T15:53:38.628+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:38 INFO CodeGenerator: Code generated in 3.243583 ms
[2023-08-31T15:53:38.635+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:38 INFO CodeGenerator: Code generated in 3.869792 ms
[2023-08-31T15:53:38.637+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:38 INFO CodeGenerator: Code generated in 1.826958 ms
[2023-08-31T15:53:38.640+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:38 INFO CodeGenerator: Code generated in 2.287792 ms
[2023-08-31T15:53:39.419+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 4072 bytes result sent to driver
[2023-08-31T15:53:39.424+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12) (192.168.1.62, executor driver, partition 0, NODE_LOCAL, 7363 bytes)
[2023-08-31T15:53:39.425+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)
[2023-08-31T15:53:39.425+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 1949 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:39.426+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2023-08-31T15:53:39.426+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: ShuffleMapStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 5,996 s
[2023-08-31T15:53:39.427+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: looking for newly runnable stages
[2023-08-31T15:53:39.428+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: running: Set(ResultStage 13)
[2023-08-31T15:53:39.428+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: waiting: Set()
[2023-08-31T15:53:39.428+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: failed: Set()
[2023-08-31T15:53:39.443+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-08-31T15:53:39.448+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO ShuffleBlockFetcherIterator: Getting 1 (26.0 KiB) non-empty blocks including 1 (26.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-08-31T15:53:39.449+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-08-31T15:53:39.459+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.557375 ms
[2023-08-31T15:53:39.464+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:39.466+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Got job 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) with 1 output partitions
[2023-08-31T15:53:39.466+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Final stage: ResultStage 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317)
[2023-08-31T15:53:39.467+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
[2023-08-31T15:53:39.467+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:39.467+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[61] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317), which has no missing parents
[2023-08-31T15:53:39.468+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 17813 bytes result sent to driver
[2023-08-31T15:53:39.468+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 49 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:39.468+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2023-08-31T15:53:39.468+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 56.4 KiB, free 2.2 GiB)
[2023-08-31T15:53:39.469+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 25.2 KiB, free 2.2 GiB)
[2023-08-31T15:53:39.469+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 192.168.1.62:54816 (size: 25.2 KiB, free: 2.4 GiB)
[2023-08-31T15:53:39.469+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:39.470+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[61] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:39.470+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2023-08-31T15:53:39.470+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 13) (192.168.1.62, executor driver, partition 0, NODE_LOCAL, 7363 bytes)
[2023-08-31T15:53:39.470+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO Executor: Running task 0.0 in stage 15.0 (TID 13)
[2023-08-31T15:53:39.471+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) finished in 4,905 s
[2023-08-31T15:53:39.471+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:39.471+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2023-08-31T15:53:39.471+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Job 12 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317, took 4,917778 s
[2023-08-31T15:53:39.473+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO ShuffleBlockFetcherIterator: Getting 1 (21.5 KiB) non-empty blocks including 1 (21.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-08-31T15:53:39.474+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-08-31T15:53:39.475+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 2.040708 ms
[2023-08-31T15:53:39.476+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 1.472 ms
[2023-08-31T15:53:39.477+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 1056.0 KiB, free 2.2 GiB)
[2023-08-31T15:53:39.478+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 1.362834 ms
[2023-08-31T15:53:39.479+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 2.2 GiB)
[2023-08-31T15:53:39.479+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 192.168.1.62:54816 (size: 17.7 KiB, free: 2.4 GiB)
[2023-08-31T15:53:39.479+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO SparkContext: Created broadcast 29 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:39.484+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.116625 ms
[2023-08-31T15:53:39.489+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 1.977542 ms
[2023-08-31T15:53:39.492+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 2.08525 ms
[2023-08-31T15:53:39.496+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO Executor: Finished task 0.0 in stage 15.0 (TID 13). 10184 bytes result sent to driver
[2023-08-31T15:53:39.497+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 13) in 27 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:39.497+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2023-08-31T15:53:39.497+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: ResultStage 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:317) finished in 0,030 s
[2023-08-31T15:53:39.498+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:39.498+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2023-08-31T15:53:39.498+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Job 13 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317, took 0,033026 s
[2023-08-31T15:53:39.499+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 1040.0 KiB, free 2.2 GiB)
[2023-08-31T15:53:39.500+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 2.2 GiB)
[2023-08-31T15:53:39.500+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 192.168.1.62:54816 (size: 8.6 KiB, free: 2.4 GiB)
[2023-08-31T15:53:39.500+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO SparkContext: Created broadcast 30 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:317
[2023-08-31T15:53:39.510+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 67108864, minimum partition size: 1048576
[2023-08-31T15:53:39.536+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 6.816375 ms
[2023-08-31T15:53:39.546+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 8.113292 ms
[2023-08-31T15:53:39.555+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 4.573667 ms
[2023-08-31T15:53:39.562+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 5.109416 ms
[2023-08-31T15:53:39.568+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.821583 ms
[2023-08-31T15:53:39.572+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.00375 ms
[2023-08-31T15:53:39.578+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 4.447292 ms
[2023-08-31T15:53:39.583+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.274958 ms
[2023-08-31T15:53:39.587+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.085583 ms
[2023-08-31T15:53:39.594+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 5.614125 ms
[2023-08-31T15:53:39.599+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.722042 ms
[2023-08-31T15:53:39.605+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.846417 ms
[2023-08-31T15:53:39.609+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.398417 ms
[2023-08-31T15:53:39.613+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 2.439125 ms
[2023-08-31T15:53:39.644+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2023-08-31T15:53:39.645+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Got job 14 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-08-31T15:53:39.645+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Final stage: ResultStage 17 (showString at NativeMethodAccessorImpl.java:0)
[2023-08-31T15:53:39.645+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
[2023-08-31T15:53:39.645+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Missing parents: List()
[2023-08-31T15:53:39.646+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[91] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-08-31T15:53:39.648+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 197.5 KiB, free 2.2 GiB)
[2023-08-31T15:53:39.649+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 56.3 KiB, free 2.2 GiB)
[2023-08-31T15:53:39.649+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 192.168.1.62:54816 (size: 56.3 KiB, free: 2.4 GiB)
[2023-08-31T15:53:39.649+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535
[2023-08-31T15:53:39.649+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[91] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-08-31T15:53:39.649+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2023-08-31T15:53:39.650+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 14) (192.168.1.62, executor driver, partition 0, NODE_LOCAL, 7363 bytes)
[2023-08-31T15:53:39.650+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO Executor: Running task 0.0 in stage 17.0 (TID 14)
[2023-08-31T15:53:39.658+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO ShuffleBlockFetcherIterator: Getting 1 (63.6 MiB) non-empty blocks including 1 (63.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-08-31T15:53:39.658+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-08-31T15:53:39.664+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:39 INFO CodeGenerator: Code generated in 3.055208 ms
[2023-08-31T15:53:40.568+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 3.717208 ms
[2023-08-31T15:53:40.572+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 2.172125 ms
[2023-08-31T15:53:40.574+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 1.26425 ms
[2023-08-31T15:53:40.596+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 4.579209 ms
[2023-08-31T15:53:40.600+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.607+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 1.923708 ms
[2023-08-31T15:53:40.610+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 1.479542 ms
[2023-08-31T15:53:40.614+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 2.996 ms
[2023-08-31T15:53:40.644+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.653+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.665+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 4.888416 ms
[2023-08-31T15:53:40.707+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.719+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.722+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.730+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 2.799708 ms
[2023-08-31T15:53:40.735+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 3.470083 ms
[2023-08-31T15:53:40.741+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.746+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.749+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.751+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.757+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 2.01 ms
[2023-08-31T15:53:40.762+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 3.20075 ms
[2023-08-31T15:53:40.767+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.772+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.776+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.779+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.781+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.786+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
[2023-08-31T15:53:40.790+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 3.631333 ms
[2023-08-31T15:53:40.794+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.797+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.800+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.804+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.809+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 192.168.1.62:54816 in memory (size: 25.2 KiB, free: 2.4 GiB)
[2023-08-31T15:53:40.809+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.825+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 3.001084 ms
[2023-08-31T15:53:40.832+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.837+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.841+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.846+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.852+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.862+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 1.860417 ms
[2023-08-31T15:53:40.866+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 2.776083 ms
[2023-08-31T15:53:40.873+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.878+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.882+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.884+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.887+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.898+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 2.137541 ms
[2023-08-31T15:53:40.905+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 4.848 ms
[2023-08-31T15:53:40.910+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.914+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.917+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.920+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.922+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.928+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.937+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 3.427042 ms
[2023-08-31T15:53:40.942+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.946+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.949+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.952+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.955+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.962+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.973+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO CodeGenerator: Code generated in 3.465583 ms
[2023-08-31T15:53:40.979+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.983+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.986+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.989+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.992+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:40.999+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:40 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.002+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.008+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO CodeGenerator: Code generated in 1.851209 ms
[2023-08-31T15:53:41.015+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.019+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.022+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.025+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.029+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.037+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.041+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.042+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.052+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO CodeGenerator: Code generated in 3.747791 ms
[2023-08-31T15:53:41.062+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO CodeGenerator: Code generated in 6.147459 ms
[2023-08-31T15:53:41.069+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO CodeGenerator: Code generated in 4.297375 ms
[2023-08-31T15:53:41.088+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO CodeGenerator: Code generated in 11.400792 ms
[2023-08-31T15:53:41.093+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO CodeGenerator: Code generated in 3.230417 ms
[2023-08-31T15:53:41.103+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO CodeGenerator: Code generated in 3.590541 ms
[2023-08-31T15:53:41.119+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.124+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.127+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.130+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.138+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.142+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.144+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.159+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.163+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.166+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.174+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.179+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.181+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.183+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.197+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.201+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.210+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.215+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.218+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.220+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.234+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.244+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.249+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.252+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.255+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.256+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.275+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.284+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.288+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.291+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.293+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.294+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.319+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 192.168.1.62:54816 in memory (size: 23.6 KiB, free: 2.4 GiB)
[2023-08-31T15:53:41.321+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 192.168.1.62:54816 in memory (size: 30.0 KiB, free: 2.4 GiB)
[2023-08-31T15:53:41.330+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.336+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.340+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.344+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.346+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.348+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.362+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.369+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.373+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.378+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.380+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.382+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.393+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.400+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.404+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.408+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.411+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.413+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.414+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.431+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.435+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.440+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.443+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.445+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.446+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.448+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.463+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.467+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.472+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.477+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.480+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.482+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.484+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.494+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.511+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.516+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.521+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.524+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.526+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.529+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.542+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.543+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.566+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.569+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.572+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.575+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.578+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.594+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.595+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.618+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.622+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.625+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.628+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.630+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.645+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.646+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.661+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.676+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.679+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.682+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.686+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.701+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.703+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.719+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.726+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.730+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.733+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.736+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.759+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.760+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.774+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.776+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.790+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.793+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.796+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.819+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.822+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.837+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.840+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.841+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.854+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.857+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.875+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.879+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.908+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.911+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.912+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.914+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.924+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.928+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.946+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.948+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.969+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.971+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.973+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.975+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.981+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.990+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:41.993+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:41 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.019+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.022+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.042+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.045+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.046+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.049+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.058+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.060+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.078+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.100+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.102+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.121+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.124+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.126+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.129+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.138+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.141+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.142+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.190+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.192+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.217+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.221+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.223+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.225+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.235+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.239+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.240+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.243+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.301+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.324+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.327+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.330+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.332+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.341+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.344+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.346+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.350+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.456+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.462+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.465+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.468+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.487+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.491+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.492+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.496+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.573+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.590+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.594+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.596+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.599+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.612+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.617+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.619+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.623+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.703+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.704+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.725+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.728+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.730+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.741+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.746+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.748+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.752+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.820+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.822+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.831+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.848+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.851+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.854+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.865+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.869+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.871+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.876+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.953+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.955+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.966+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.969+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.990+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:42.993+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:42 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.008+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.012+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.014+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.019+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.142+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.144+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.158+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.161+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.162+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.186+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.200+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.205+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.207+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.211+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.318+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.320+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.332+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.336+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.338+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.339+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.373+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.378+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.380+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.385+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.481+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.484+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.497+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.502+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.503+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.505+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.508+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.536+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.539+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.546+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.649+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.652+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.674+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.681+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.687+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.690+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.690+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.697+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.737+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.745+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.868+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.873+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.891+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.896+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.898+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.900+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.901+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.910+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.920+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:43.947+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:43 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.074+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.076+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.092+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.098+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.100+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.101+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.103+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.113+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.128+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.346+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.349+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.364+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.370+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.372+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.373+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.375+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.383+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.398+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.400+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.409+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.412+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.432+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.437+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.440+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.443+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.447+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.456+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.470+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.473+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.475+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.489+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.509+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.515+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.518+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.521+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.523+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.536+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.552+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.556+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.560+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.562+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.612+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.619+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.621+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.624+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.626+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.638+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.661+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.664+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.668+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.672+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.695+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.697+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.701+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.704+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.716+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.741+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.744+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.748+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.752+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.760+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
[2023-08-31T15:53:44.813+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO Executor: Finished task 0.0 in stage 17.0 (TID 14). 10061 bytes result sent to driver
[2023-08-31T15:53:44.813+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 14) in 5163 ms on 192.168.1.62 (executor driver) (1/1)
[2023-08-31T15:53:44.814+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2023-08-31T15:53:44.814+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO DAGScheduler: ResultStage 17 (showString at NativeMethodAccessorImpl.java:0) finished in 5,168 s
[2023-08-31T15:53:44.815+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-08-31T15:53:44.815+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2023-08-31T15:53:44.815+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO DAGScheduler: Job 14 finished: showString at NativeMethodAccessorImpl.java:0, took 5,170443 s
[2023-08-31T15:53:44.828+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO CodeGenerator: Code generated in 7.039125 ms
[2023-08-31T15:53:44.832+0200] {spark_submit.py:491} INFO - +---------+-------------------+--------------------+--------------------+------------+-----------+-------------------------+------+----------------------------+--------------------------------+-----------------------+---------------------------+--------------------------+------------------+----------------------+--------------------------+-------------------------+-----------------+------------------------+----------------------------+---------------------------+-------------------+--------------------------------+--------------------------------+-----------------------+----------------------------+
[2023-08-31T15:53:44.832+0200] {spark_submit.py:491} INFO - | order_id|order_creation_time|           device_id|       creation_time|  error_code|status_code|conn_status_creation_time|status|pollingCT_orderCT_difference|conn_statusCT_orderCT_difference|total_poll_events_-180s|count_typeof_status_c_-180s|count_typeof_error_c_-180s|ok_responses_-180s|total_poll_events_180s|count_typeof_status_c_180s|count_typeof_error_c_180s|ok_responses_180s|total_poll_events_-3600s|count_typeof_status_c_-3600s|count_typeof_error_c_-3600s|ok_responses_-3600s|immed_preceding_polling_event_CT|immed_following_polling_event_CT|most_recent_conn_status|most_recent_conn_status_time|
[2023-08-31T15:53:44.833+0200] {spark_submit.py:491} INFO - +---------+-------------------+--------------------+--------------------+------------+-----------+-------------------------+------+----------------------------+--------------------------------+-----------------------+---------------------------+--------------------------+------------------+----------------------+--------------------------+-------------------------+-----------------+------------------------+----------------------------+---------------------------+-------------------+--------------------------------+--------------------------------+-----------------------+----------------------------+
[2023-08-31T15:53:44.833+0200] {spark_submit.py:491} INFO - |102458251|2020-02-26 00:49:57|d0bc996f-72d2-4ec...|2020-02-26 05:29:...|        null|        200|     2020-02-26 02:08:...|ONLINE|                       16777|                            4736|                    114|                      , 200|                         0|               114|                   108|                     , 200|                        0|              108|                    1488|                    , 200, 0|            ECONNABORTED, 0|               1464|             2020-02-26 00:00:02|             2020-02-26 00:49:58|                 ONLINE|         2020-02-26 00:44:18|
[2023-08-31T15:53:44.833+0200] {spark_submit.py:491} INFO - |102463944|2020-02-26 01:32:28|d0bc996f-72d2-4ec...|2020-02-26 05:29:...|        null|        200|     2020-02-26 02:08:...|ONLINE|                       14226|                            2185|                     66|                      , 200|                         0|                66|                    66|                     , 200|                        0|               66|                    1662|                    , 200, 0|            ECONNABORTED, 0|               1638|             2020-02-26 00:00:02|             2020-02-26 01:32:44|                 ONLINE|         2020-02-26 00:44:18|
[2023-08-31T15:53:44.833+0200] {spark_submit.py:491} INFO - |102472308|2020-02-26 03:01:56|d0bc996f-72d2-4ec...|2020-02-26 05:29:...|        null|        200|     2020-02-26 02:08:...|ONLINE|                        8858|                           -3183|                     30|                   , 200, 0|           ECONNABORTED, 0|                24|                    18|                  , 200, 0|          ECONNABORTED, 0|                6|                     768|                    , 200, 0|       ECONNABORTED, GEN...|                738|             2020-02-26 00:00:02|             2020-02-26 03:02:21|                 ONLINE|         2020-02-26 00:44:18|
[2023-08-31T15:53:44.833+0200] {spark_submit.py:491} INFO - |102495832|2020-02-26 10:59:59|eb870f2d-f8c7-4be...|2020-02-26 11:16:...|        null|        200|     2020-02-26 09:14:...|ONLINE|                         963|                           -6355|                     24|                      , 200|                         0|                24|                    22|                     , 200|                        0|               22|                     386|                       , 200|                          0|                386|             2020-02-26 10:03:51|             2020-02-26 11:00:11|                 ONLINE|         2020-02-26 09:13:57|
[2023-08-31T15:53:44.833+0200] {spark_submit.py:491} INFO - |102496079|2020-02-26 11:05:23|eb870f2d-f8c7-4be...|2020-02-26 11:16:...|        null|        200|     2020-02-26 09:14:...|ONLINE|                         639|                           -6679|                     22|                      , 200|                         0|                22|                    24|                     , 200|                        0|               24|                     412|                       , 200|                          0|                412|             2020-02-26 10:03:51|             2020-02-26 11:05:25|                 ONLINE|         2020-02-26 09:13:57|
[2023-08-31T15:53:44.833+0200] {spark_submit.py:491} INFO - |102500685|2020-02-26 11:31:02|a8d153dc-2294-444...|2020-02-26 23:59:...|        null|        200|     2020-02-26 11:30:...|ONLINE|                       44933|                             -48|                      4|                   , 200, 0|           ECONNABORTED, 0|                 2|                    14|                  , 200, 0|          ECONNABORTED, 0|               12|                     288|                    , 200, 0|            ECONNABORTED, 0|                284|             2020-02-26 00:21:52|             2020-02-26 11:31:34|                 ONLINE|         2020-02-26 11:30:12|
[2023-08-31T15:53:44.834+0200] {spark_submit.py:491} INFO - |102500992|2020-02-26 11:33:03|4a771d87-1cc3-453...|2020-02-26 23:59:...|        null|        200|     2020-02-26 10:36:...|ONLINE|                       44809|                           -3408|                     88|                      , 200|                         0|                88|                    88|                     , 200|                        0|               88|                    1624|                       , 200|                          0|               1624|             2020-02-26 00:00:09|             2020-02-26 11:33:15|                 ONLINE|         2020-02-26 03:17:43|
[2023-08-31T15:53:44.834+0200] {spark_submit.py:491} INFO - |102501158|2020-02-26 11:34:32|82fb1d95-c894-449...|2020-02-26 23:02:...|        null|        200|     2020-02-26 18:20:...|ONLINE|                       41302|                           24347|                     48|                      , 200|                         0|                48|                    44|                     , 200|                        0|               44|                     892|                       , 200|                          0|                892|             2020-02-26 10:13:36|             2020-02-26 11:34:45|                 ONLINE|         2020-02-26 09:29:17|
[2023-08-31T15:53:44.834+0200] {spark_submit.py:491} INFO - |102501936|2020-02-26 11:38:44|055f5c23-c398-4b1...|2020-02-26 23:54:...|ECONNABORTED|          0|     2020-02-26 22:24:...|ONLINE|                       44120|                           38762|                    814|                      , 200|                         0|               814|                   814|                     , 200|                        0|              814|                   12580|                    , 200, 0|       ECONNABORTED, GEN...|              12432|             2020-02-26 10:50:27|             2020-02-26 11:38:46|                 ONLINE|         2020-02-26 11:07:20|
[2023-08-31T15:53:44.834+0200] {spark_submit.py:491} INFO - |102502443|2020-02-26 11:45:26|a8d153dc-2294-444...|2020-02-26 23:59:...|        null|        200|     2020-02-26 11:30:...|ONLINE|                       44069|                            -912|                     18|                      , 200|                         0|                18|                    14|                  , 200, 0|          ECONNABORTED, 0|               10|                     320|                    , 200, 0|            ECONNABORTED, 0|                308|             2020-02-26 00:21:52|             2020-02-26 11:45:29|                 ONLINE|         2020-02-26 11:30:12|
[2023-08-31T15:53:44.834+0200] {spark_submit.py:491} INFO - +---------+-------------------+--------------------+--------------------+------------+-----------+-------------------------+------+----------------------------+--------------------------------+-----------------------+---------------------------+--------------------------+------------------+----------------------+--------------------------+-------------------------+-----------------+------------------------+----------------------------+---------------------------+-------------------+--------------------------------+--------------------------------+-----------------------+----------------------------+
[2023-08-31T15:53:44.834+0200] {spark_submit.py:491} INFO - only showing top 10 rows
[2023-08-31T15:53:44.835+0200] {spark_submit.py:491} INFO - 
[2023-08-31T15:53:44.851+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO SparkContext: Invoking stop() from shutdown hook
[2023-08-31T15:53:44.851+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2023-08-31T15:53:44.856+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO SparkUI: Stopped Spark web UI at http://192.168.1.62:4040
[2023-08-31T15:53:44.860+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-08-31T15:53:44.872+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO MemoryStore: MemoryStore cleared
[2023-08-31T15:53:44.872+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO BlockManager: BlockManager stopped
[2023-08-31T15:53:44.873+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-08-31T15:53:44.874+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-08-31T15:53:44.877+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO SparkContext: Successfully stopped SparkContext
[2023-08-31T15:53:44.878+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ShutdownHookManager: Shutdown hook called
[2023-08-31T15:53:44.878+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ShutdownHookManager: Deleting directory /private/var/folders/bq/3fzm19j96pq0b42q_ghqb6j40000gn/T/spark-a916fa7e-c91c-4bf1-9bd7-fb557ae9fd3e
[2023-08-31T15:53:44.880+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ShutdownHookManager: Deleting directory /private/var/folders/bq/3fzm19j96pq0b42q_ghqb6j40000gn/T/spark-a916fa7e-c91c-4bf1-9bd7-fb557ae9fd3e/pyspark-63d3be3c-fb5c-4335-8899-b00dd94dd50d
[2023-08-31T15:53:44.883+0200] {spark_submit.py:491} INFO - 23/08/31 15:53:44 INFO ShutdownHookManager: Deleting directory /private/var/folders/bq/3fzm19j96pq0b42q_ghqb6j40000gn/T/spark-0dbd6fb6-75a9-49b5-bceb-191e76ef2512
[2023-08-31T15:53:44.922+0200] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=de_test_workflow, task_id=sparkScript, execution_date=20230831T135325, start_date=20230831T135327, end_date=20230831T135344
[2023-08-31T15:53:44.939+0200] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2023-08-31T15:53:44.959+0200] {taskinstance.py:2784} INFO - 1 downstream tasks scheduled from follow-on schedule check
